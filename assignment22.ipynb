{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1465a09c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import HTMLHeaderTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_huggingface import HuggingFaceEmbeddings \n",
    "from langchain_chroma import Chroma \n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import create_retrieval_chain, create_history_aware_retriever\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain \n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "# Step 1: Load and split blog content\n",
    "loader = WebBaseLoader(web_path=\"https://blog.mdturp.ch/posts/2024-04-05-visual_guide_to_vision_transformer.html\", \n",
    "                       bs_kwargs=dict(parse_only=bs4.SoupStrainer(\n",
    "                           class_=(\"main\")\n",
    "                       )))\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c59ace9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://blog.mdturp.ch/posts/2024-04-05-visual_guide_to_vision_transformer.html'}, page_content=\"A Visual Guide to Vision Transformers \\u200bThis is a visual guide to Vision Transformers (ViTs), a class of deep learning models that have achieved state-of-the-art performance on image classification tasks. Vision Transformers apply the transformer architecture, originally designed for natural language processing (NLP), to image data. This guide will walk you through the key components of Vision Transformers in a scroll story format, using visualizations and simple explanations to help you understand how these models work and how the flow of the data through the model looks like.Translations \\u200bLanguageLinkTranslated by游썷릖 KoreanLinkJunghwan ParkPlease enjoy and start scrolling!0) Lets start with the data \\u200bLike normal convolutional neural networks, vision transformers are trained in a supervised manner. This means that the model is trained on a dataset of images and their corresponding labels.1) Focus on one data point To get a better understanding of what happens inside a vision transformer lets focus on a single data point (batch size of 1). And lets ask the question: How is this data point prepared in order to be consumed by a transformer?2) Forget the label for the momentThe label will become more relevant later. For now the only thing that we are left with is a single image.3) Create patches of the imageTo prepare the image for the use inside the transformer we divide the image into equally sized patches of size p x p.4) Flatting of the images patchesThe patches are now flattened into vectors of dimension  p'= p*c  where  p  is the size of the patch and  c  is the number of channels.5) Creating patch embeddingsThese image patch vectors are now encoded using a linear transformation. The resulting Patch Embedding Vector has a fixed size d.6) Embedding all patchesNow that we have embedded our image patches into vectors of fixed size, we are left with an array of size n x d where  n  is the the number of image patches and  d  is the size of the patch embedding7) Appending a classification tokenIn order for us to effectively train our model we extend the array of patch embeddings by an additional vector called classification token (cls token). This vector is a learnable parameter of the network and is randomly initialized. Note: We only have one cls token and we append the same vector for all data points.8) Add positional embedding VectorsCurrently our  patch embeddings  have no positional information associated with them. We remedy that by adding a learnable randomly initialized positional embedding vector to all our patch embeddings. We also add a such a positional embedding vector to our  classification token.9) Transformer InputAfter the positional embedding vectors have been added we are left with an array of size  (n+1) x d . This will be our input for the transformer which will be explained in greater detail in the next steps10.1) Transformer: QKV Creation Our transformer input patch embedding vectors are linearly embedded into multiple large vectors. These new vectors are than separated into three equal sized parts. The  Q - Query Vector, the  K - Key Vector  and the  V - Value Vector . We will have (n+1) of a all of those vectors. 10.2) Transformer: Attention Score Calculation To calculate our attention scores A we will now multiply all of our query vectors Q with all of our key vectors K.10.3)Transformer: Attention Score Matrix Now that we have the attention score matrix A we apply a `softmax` function to every row such that every row sums up to 1.10.4)Transformer: Aggregated Contextual Information Calculation To calculate the aggregated contextual information for the first patch embedding vector. We focus on the first row of the attention matrix. And use the entires as weights for our Value Vectors V. The result is our aggregated contextual information vector for the first image patch embedding.10.5)Transformer: Aggregated Contextual Information for every patchNow we repeat this process for every row of our attention score matrix and the result will be N+1 aggregated contextual information vectors. One for every patch + one for the classification token. This steps concludes our first Attention Head.10.6)Transformer: Multi-Head AttentionNow because we are dealing multi head attention we repeat the entire process from step 10.1 - 10-5 again with a different QKV mapping. For our explanatory setup we assume 2 Heads but typically a VIT has many more. In the end this results in multiple Aggregated contextual information vectors.10.7)Transformer: Last Attention Layer StepThese heads are stacked together and are mapped to vectors of size d which was the same size as our patch embeddings had.10.8)Transformer: Attention Layer ResultThe previous step concluded the attention layer and we are left with the same amount of embeddings of exactly the same size as we used as input.10.9)Transformer: Residual connectionsTransformers make heavy use of residual connections which simply means adding the input of the previous layer to the output the current layer. This is also something that we will do now. 10.10)Transformer: Residual connection ResultThe addition results in vectors of the same size. 10.11)Transformer: Feed Forward Network Now these outputs are feed through a feed forward neural network with non linear activation functions10.12)Transformer: Final Result After the transformer step there is another residual connections which we will skip here for brevity. And so the last step concluded the transformer layer. In the end the transformer produced outputs of the same size as input.11) Repeat Transformers Repeat the entire transformer calculation Steps 10.1 - Steps 10.12 for the Transformer several times e.g. 6 times.12) Identify Classification token outputLast step is to identify the classification token output. This vector will be used in the final step of our Vision Transformer journey.13) Final Step: Predicting classification probabilitiesIn the final and last step we use this classification output token and another fully connected neural network to predict the classification probabilities of our input image.14) Training of the Vision Transformer \\u200bWe train the Vision Transformer using a standard cross-entropy loss function, which compares the predicted class probabilities with the true class labels. The model is trained using backpropagation and gradient descent, updating the model parameters to minimize the loss function.Conclusion \\u200bIn this visual guide, we have walked through the key components of Vision Transformers, from the data preparation to the training of the model. We hope this guide has helped you understand how Vision Transformers work and how they can be used to classify images.I prepared this little Colab Notebook to help you understand the Vision Transformer even better. Please have look for the 'Blogpost' comment. The code was taken from @lucidrains great VIT Pytorch implementation be sure to checkout his work.If you have any questions or feedback, please feel free to reach out to me. Thank you for reading!Acknowledgements \\u200bVIT Pytorch implementationAll images have been taken from Wikipedia and are licensed under the Creative Commons Attribution-Share Alike 4.0 International license.\")]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48f0ea2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='[ ] [ ] [ ]  \\nSkip to content  \\n[ ] [ MDTURP ] [ ]  \\n[ ] [ ]  \\n[ [ ] [ ] ]  \\nMain Navigation  \\n[ ]  \\nHome  \\n[ ]  \\nAbout  \\n[ ]  \\n[ ]  \\nGitHub  \\nX  \\nLinkedIn  \\n[ [ ] ]  \\nAppearance  \\n[ ]  \\n[ ]  \\nGitHub  \\nX  \\nLinkedIn  \\nReturn to top  \\n[ ] [ ]  \\n[ ] [ ] [ ] [ ] [ ] [ ]  \\nOn this page  \\nTable of Contents for current page  \\n[ ]  \\n[ ] [ ]'),\n",
       " Document(metadata={'Header 1': 'A Visual Guide to Vision Transformers'}, page_content='A Visual Guide to Vision Transformers'),\n",
       " Document(metadata={'Header 1': 'A Visual Guide to Vision Transformers'}, page_content='칙\\x80\\x8b  \\nThis is a visual guide to Vision Transformers (ViTs), a class of deep learning models that have achieved state-of-the-art performance on image classification tasks. Vision Transformers apply the transformer architecture, originally designed for natural language processing (NLP), to image data. This guide will walk you through the key components of Vision Transformers in a scroll story format, using visualizations and simple explanations to help you understand how these models work and how the flow of the data through the model looks like.'),\n",
       " Document(metadata={'Header 1': 'A Visual Guide to Vision Transformers', 'Header 4': 'Translations'}, page_content='Translations'),\n",
       " Document(metadata={'Header 1': 'A Visual Guide to Vision Transformers', 'Header 4': 'Translations'}, page_content='칙\\x80\\x8b  \\nLanguage  \\nLink  \\nTranslated by  \\n칧\\x9f\\x87춿칧\\x9f\\x87췅 Korean  \\nLink  \\nJunghwan Park  \\nPlease enjoy and start scrolling!'),\n",
       " Document(metadata={'Header 1': 'A Visual Guide to Vision Transformers', 'Header 3': '0) Lets start with the data'}, page_content='0) Lets start with the data'),\n",
       " Document(metadata={'Header 1': 'A Visual Guide to Vision Transformers', 'Header 3': '0) Lets start with the data'}, page_content='칙\\x80\\x8b  \\nLike normal convolutional neural networks, vision transformers are trained in a supervised manner. This means that the model is trained on a dataset of images and their corresponding labels.'),\n",
       " Document(metadata={'Header 1': 'A Visual Guide to Vision Transformers', 'Header 3': '1) Focus on one data point'}, page_content='1) Focus on one data point'),\n",
       " Document(metadata={'Header 1': 'A Visual Guide to Vision Transformers', 'Header 3': '1) Focus on one data point'}, page_content='To get a better understanding of what happens inside a vision transformer lets focus on a single data point (batch size of 1). And lets ask the question: How is this data point prepared in order to be consumed by a transformer?'),\n",
       " Document(metadata={'Header 1': 'A Visual Guide to Vision Transformers', 'Header 3': '2) Forget the label for the moment'}, page_content='2) Forget the label for the moment'),\n",
       " Document(metadata={'Header 1': 'A Visual Guide to Vision Transformers', 'Header 3': '2) Forget the label for the moment'}, page_content='The label will become more relevant later. For now the only thing that we are left with is a single image.'),\n",
       " Document(metadata={'Header 1': 'A Visual Guide to Vision Transformers', 'Header 3': '3) Create patches of the image'}, page_content='3) Create patches of the image'),\n",
       " Document(metadata={'Header 1': 'A Visual Guide to Vision Transformers', 'Header 3': '3) Create patches of the image'}, page_content='To prepare the image for the use inside the transformer we divide the image into equally sized patches of size .  \\np x p'),\n",
       " Document(metadata={'Header 1': 'A Visual Guide to Vision Transformers', 'Header 3': '4) Flatting of the images patches'}, page_content='4) Flatting of the images patches'),\n",
       " Document(metadata={'Header 1': 'A Visual Guide to Vision Transformers', 'Header 3': '4) Flatting of the images patches'}, page_content=\"The patches are now flattened into vectors of dimension where is the size of the patch and is the number of channels.  \\np'= p츽*c  \\np  \\nc\"),\n",
       " Document(metadata={'Header 1': 'A Visual Guide to Vision Transformers', 'Header 3': '5) Creating patch embeddings'}, page_content='5) Creating patch embeddings'),\n",
       " Document(metadata={'Header 1': 'A Visual Guide to Vision Transformers', 'Header 3': '5) Creating patch embeddings'}, page_content='These image patch vectors are now encoded using a linear transformation. The resulting has a fixed size .  \\nPatch Embedding Vector  \\nd'),\n",
       " Document(metadata={'Header 1': 'A Visual Guide to Vision Transformers', 'Header 3': '6) Embedding all patches'}, page_content='6) Embedding all patches'),\n",
       " Document(metadata={'Header 1': 'A Visual Guide to Vision Transformers', 'Header 3': '6) Embedding all patches'}, page_content='Now that we have embedded our image patches into vectors of fixed size, we are left with an array of size where is the the number of image patches and is the size of the  \\nn x d  \\nn  \\nd  \\npatch embedding'),\n",
       " Document(metadata={'Header 1': 'A Visual Guide to Vision Transformers', 'Header 3': '7) Appending a classification token'}, page_content='7) Appending a classification token'),\n",
       " Document(metadata={'Header 1': 'A Visual Guide to Vision Transformers', 'Header 3': '7) Appending a classification token'}, page_content='In order for us to effectively train our model we extend the array of patch embeddings by an additional vector called . This vector is a learnable parameter of the network and is randomly initialized. Note: We only have one cls token and we append the same vector for all data points.  \\nclassification token (cls token)'),\n",
       " Document(metadata={'Header 1': 'A Visual Guide to Vision Transformers', 'Header 3': '8) Add positional embedding Vectors'}, page_content='8) Add positional embedding Vectors'),\n",
       " Document(metadata={'Header 1': 'A Visual Guide to Vision Transformers', 'Header 3': '8) Add positional embedding Vectors'}, page_content='Currently our have no positional information associated with them. We remedy that by adding a learnable randomly initialized to all our patch embeddings. We also add a such a positional embedding vector to our .  \\npatch embeddings  \\npositional embedding vector  \\nclassification token'),\n",
       " Document(metadata={'Header 1': 'A Visual Guide to Vision Transformers', 'Header 3': '9) Transformer Input'}, page_content='9) Transformer Input'),\n",
       " Document(metadata={'Header 1': 'A Visual Guide to Vision Transformers', 'Header 3': '9) Transformer Input'}, page_content='After the positional embedding vectors have been added we are left with an array of size . This will be our input for the transformer which will be explained in greater detail in the next steps  \\n(n+1) x d'),\n",
       " Document(metadata={'Header 1': 'A Visual Guide to Vision Transformers', 'Header 3': '10.1) Transformer: QKV Creation'}, page_content='10.1) Transformer: QKV Creation'),\n",
       " Document(metadata={'Header 1': 'A Visual Guide to Vision Transformers', 'Header 3': '10.1) Transformer: QKV Creation'}, page_content='Our transformer input patch embedding vectors are linearly embedded into multiple large vectors. These new vectors are than separated into three equal sized parts. The , the and the . We will have (n+1) of a all of those vectors.  \\nQ - Query Vector  \\nK - Key Vector  \\nV - Value Vector'),\n",
       " Document(metadata={'Header 1': 'A Visual Guide to Vision Transformers', 'Header 3': '10.2) Transformer: Attention Score Calculation'}, page_content='10.2) Transformer: Attention Score Calculation'),\n",
       " Document(metadata={'Header 1': 'A Visual Guide to Vision Transformers', 'Header 3': '10.2) Transformer: Attention Score Calculation'}, page_content='To calculate our attention scores A we will now multiply all of our query vectors Q with all of our key vectors K.'),\n",
       " Document(metadata={'Header 1': 'A Visual Guide to Vision Transformers', 'Header 3': '10.3)Transformer: Attention Score Matrix'}, page_content='10.3)Transformer: Attention Score Matrix'),\n",
       " Document(metadata={'Header 1': 'A Visual Guide to Vision Transformers', 'Header 3': '10.3)Transformer: Attention Score Matrix'}, page_content='Now that we have the attention score matrix A we apply a `softmax` function to every row such that every row sums up to 1.'),\n",
       " Document(metadata={'Header 1': 'A Visual Guide to Vision Transformers', 'Header 3': '10.4)Transformer: Aggregated Contextual Information Calculation'}, page_content='10.4)Transformer: Aggregated Contextual Information Calculation'),\n",
       " Document(metadata={'Header 1': 'A Visual Guide to Vision Transformers', 'Header 3': '10.4)Transformer: Aggregated Contextual Information Calculation'}, page_content='To calculate the for the first patch embedding vector. We focus on the of the attention matrix. And use the entires as weights for our . The result is our vector for the first image patch embedding.  \\naggregated contextual information  \\nfirst row  \\nValue Vectors V  \\naggregated contextual information'),\n",
       " Document(metadata={'Header 1': 'A Visual Guide to Vision Transformers', 'Header 3': '10.5)Transformer: Aggregated Contextual Information for every patch'}, page_content='10.5)Transformer: Aggregated Contextual Information for every patch'),\n",
       " Document(metadata={'Header 1': 'A Visual Guide to Vision Transformers', 'Header 3': '10.5)Transformer: Aggregated Contextual Information for every patch'}, page_content='Now we repeat this process for every row of our attention score matrix and the result will be N+1 aggregated contextual information vectors. One for every patch + one for the classification token. This steps concludes our first Attention Head.'),\n",
       " Document(metadata={'Header 1': 'A Visual Guide to Vision Transformers', 'Header 3': '10.6)Transformer: Multi-Head Attention'}, page_content='10.6)Transformer: Multi-Head Attention'),\n",
       " Document(metadata={'Header 1': 'A Visual Guide to Vision Transformers', 'Header 3': '10.6)Transformer: Multi-Head Attention'}, page_content='Now because we are dealing multi head attention we repeat the entire process from step again with a different QKV mapping. For our explanatory setup we assume 2 Heads but typically a VIT has many more. In the end this results in multiple Aggregated contextual information vectors.  \\n10.1 - 10-5'),\n",
       " Document(metadata={'Header 1': 'A Visual Guide to Vision Transformers', 'Header 3': '10.7)Transformer: Last Attention Layer Step'}, page_content='10.7)Transformer: Last Attention Layer Step'),\n",
       " Document(metadata={'Header 1': 'A Visual Guide to Vision Transformers', 'Header 3': '10.7)Transformer: Last Attention Layer Step'}, page_content='These heads are stacked together and are mapped to vectors of size which was the same size as our patch embeddings had.  \\nd'),\n",
       " Document(metadata={'Header 1': 'A Visual Guide to Vision Transformers', 'Header 3': '10.8)Transformer: Attention Layer Result'}, page_content='10.8)Transformer: Attention Layer Result'),\n",
       " Document(metadata={'Header 1': 'A Visual Guide to Vision Transformers', 'Header 3': '10.8)Transformer: Attention Layer Result'}, page_content='The previous step concluded the attention layer and we are left with the same amount of embeddings of as we used as input.  \\nexactly the same size'),\n",
       " Document(metadata={'Header 1': 'A Visual Guide to Vision Transformers', 'Header 3': '10.9)Transformer: Residual connections'}, page_content='10.9)Transformer: Residual connections'),\n",
       " Document(metadata={'Header 1': 'A Visual Guide to Vision Transformers', 'Header 3': '10.9)Transformer: Residual connections'}, page_content='Transformers make heavy use of which simply means adding the input of the previous layer to the output the current layer. This is also something that we will do now.  \\nresidual connections'),\n",
       " Document(metadata={'Header 1': 'A Visual Guide to Vision Transformers', 'Header 3': '10.10)Transformer: Residual connection Result'}, page_content='10.10)Transformer: Residual connection Result'),\n",
       " Document(metadata={'Header 1': 'A Visual Guide to Vision Transformers', 'Header 3': '10.10)Transformer: Residual connection Result'}, page_content='The addition results in vectors of the same size.'),\n",
       " Document(metadata={'Header 1': 'A Visual Guide to Vision Transformers', 'Header 3': '10.11)Transformer: Feed Forward Network'}, page_content='10.11)Transformer: Feed Forward Network'),\n",
       " Document(metadata={'Header 1': 'A Visual Guide to Vision Transformers', 'Header 3': '10.11)Transformer: Feed Forward Network'}, page_content='Now these outputs are feed through a feed forward neural network with non linear activation functions'),\n",
       " Document(metadata={'Header 1': 'A Visual Guide to Vision Transformers', 'Header 3': '10.12)Transformer: Final Result'}, page_content='10.12)Transformer: Final Result'),\n",
       " Document(metadata={'Header 1': 'A Visual Guide to Vision Transformers', 'Header 3': '10.12)Transformer: Final Result'}, page_content='After the transformer step there is another residual connections which we will skip here for brevity. And so the last step concluded the transformer layer. In the end the transformer produced outputs of the same size as input.'),\n",
       " Document(metadata={'Header 1': 'A Visual Guide to Vision Transformers', 'Header 3': '11) Repeat Transformers'}, page_content='11) Repeat Transformers'),\n",
       " Document(metadata={'Header 1': 'A Visual Guide to Vision Transformers', 'Header 3': '11) Repeat Transformers'}, page_content='Repeat the entire transformer calculation for the Transformer several times e.g. 6 times.  \\nSteps 10.1 - Steps 10.12'),\n",
       " Document(metadata={'Header 1': 'A Visual Guide to Vision Transformers', 'Header 3': '12) Identify Classification token output'}, page_content='12) Identify Classification token output'),\n",
       " Document(metadata={'Header 1': 'A Visual Guide to Vision Transformers', 'Header 3': '12) Identify Classification token output'}, page_content='Last step is to identify the classification token output. This vector will be used in the final step of our Vision Transformer journey.'),\n",
       " Document(metadata={'Header 1': 'A Visual Guide to Vision Transformers', 'Header 3': '13) Final Step: Predicting classification probabilities'}, page_content='13) Final Step: Predicting classification probabilities'),\n",
       " Document(metadata={'Header 1': 'A Visual Guide to Vision Transformers', 'Header 3': '13) Final Step: Predicting classification probabilities'}, page_content='In the final and last step we use this classification output token and another fully connected neural network to predict the classification probabilities of our input image.'),\n",
       " Document(metadata={'Header 1': 'A Visual Guide to Vision Transformers', 'Header 3': '14) Training of the Vision Transformer'}, page_content='14) Training of the Vision Transformer'),\n",
       " Document(metadata={'Header 1': 'A Visual Guide to Vision Transformers', 'Header 3': '14) Training of the Vision Transformer'}, page_content='칙\\x80\\x8b  \\nWe train the Vision Transformer using a standard cross-entropy loss function, which compares the predicted class probabilities with the true class labels. The model is trained using backpropagation and gradient descent, updating the model parameters to minimize the loss function.'),\n",
       " Document(metadata={'Header 1': 'A Visual Guide to Vision Transformers', 'Header 2': 'Conclusion'}, page_content='Conclusion'),\n",
       " Document(metadata={'Header 1': 'A Visual Guide to Vision Transformers', 'Header 2': 'Conclusion'}, page_content=\"칙\\x80\\x8b  \\nIn this visual guide, we have walked through the key components of Vision Transformers, from the data preparation to the training of the model. We hope this guide has helped you understand how Vision Transformers work and how they can be used to classify images.  \\nI prepared this little to help you understand the Vision Transformer even better. Please have look for the 'Blogpost' comment. The code was taken from @lucidrains great be sure to checkout his work.  \\nColab Notebook  \\nVIT Pytorch implementation  \\nIf you have any questions or feedback, please feel free to reach out to me. Thank you for reading!\"),\n",
       " Document(metadata={'Header 1': 'A Visual Guide to Vision Transformers', 'Header 2': 'Acknowledgements'}, page_content='Acknowledgements'),\n",
       " Document(metadata={}, page_content='칙\\x80\\x8b  \\nVIT Pytorch implementation  \\nAll images have been taken from Wikipedia and are licensed under the Creative Commons Attribution-Share Alike 4.0 International license.  \\n[ ]  \\nCreated by M. Dennis Turp  \\nwindow.__VP_HASH_MAP__=JSON.parse(\"{\\\\\"about.md\\\\\":\\\\\"BBUktiic\\\\\",\\\\\"api-examples.md\\\\\":\\\\\"CaJ15qOX\\\\\",\\\\\"index.md\\\\\":\\\\\"DV5fnS6Q\\\\\",\\\\\"markdown-examples.md\\\\\":\\\\\"BWwyZYKO\\\\\",\\\\\"posts_2024-01-17-image-superresolution.md\\\\\":\\\\\"DDlBKZIF\\\\\",\\\\\"posts_2024-01-24-low-light-image-enhancement.md\\\\\":\\\\\"BBxo7M7z\\\\\",\\\\\"posts_2024-04-05-visual_guide_to_vision_transformer.md\\\\\":\\\\\"DJ9_jBQA\\\\\",\\\\\"posts_draft-2025-01-06-understanding-lora-and-dora-a-visual-guide-to-efficient-model-adaptation.md\\\\\":\\\\\"DWM6wNJV\\\\\"}\");window.__VP_SITE_DATA__=JSON.parse(\"{\\\\\"lang\\\\\":\\\\\"en-US\\\\\",\\\\\"dir\\\\\":\\\\\"ltr\\\\\",\\\\\"title\\\\\":\\\\\"MDTURP\\\\\",\\\\\"description\\\\\":\\\\\"Personal blog of M.Dennis Turp\\\\\",\\\\\"base\\\\\":\\\\\"/\\\\\",\\\\\"head\\\\\":[],\\\\\"router\\\\\":{\\\\\"prefetchLinks\\\\\":true},\\\\\"appearance\\\\\":true,\\\\\"themeConfig\\\\\":{\\\\\"nav\\\\\":[{\\\\\"text\\\\\":\\\\\"Home\\\\\",\\\\\"link\\\\\":\\\\\"/\\\\\"},{\\\\\"text\\\\\":\\\\\"About\\\\\",\\\\\"link\\\\\":\\\\\"/about\\\\\"}],\\\\\"sidebar\\\\\":[],\\\\\"socialLinks\\\\\":[{\\\\\"icon\\\\\":\\\\\"github\\\\\",\\\\\"link\\\\\":\\\\\"https://github.com/mdturp\\\\\"},{\\\\\"icon\\\\\":\\\\\"x\\\\\",\\\\\"link\\\\\":\\\\\"https://twitter.com/Dennis_Turp\\\\\"},{\\\\\"icon\\\\\":{\\\\\"svg\\\\\":\\\\\"<svg aria-label=\\\\\\\\\\\\\"Threads\\\\\\\\\\\\\" viewBox=\\\\\\\\\\\\\"0 0 192 192\\\\\\\\\\\\\" xmlns=\\\\\\\\\\\\\"http://www.w3.org/2000/svg\\\\\\\\\\\\\"><path class=\\\\\\\\\\\\\"x19hqcy\\\\\\\\\\\\\" d=\\\\\\\\\\\\\"M141.537 88.9883C140.71 88.5919 139.87 88.2104 139.019 87.8451C137.537 60.5382 122.616 44.905 97.5619 44.745C97.4484 44.7443 97.3355 44.7443 97.222 44.7443C82.2364 44.7443 69.7731 51.1409 62.102 62.7807L75.881 72.2328C81.6116 63.5383 90.6052 61.6848 97.2286 61.6848C97.3051 61.6848 97.3819 61.6848 97.4576 61.6855C105.707 61.7381 111.932 64.1366 115.961 68.814C118.893 72.2193 120.854 76.925 121.825 82.8638C114.511 81.6207 106.601 81.2385 98.145 81.7233C74.3247 83.0954 59.0111 96.9879 60.0396 116.292C60.5615 126.084 65.4397 134.508 73.775 140.011C80.8224 144.663 89.899 146.938 99.3323 146.423C111.79 145.74 121.563 140.987 128.381 132.296C133.559 125.696 136.834 117.143 138.28 106.366C144.217 109.949 148.617 114.664 151.047 120.332C155.179 129.967 155.42 145.8 142.501 158.708C131.182 170.016 117.576 174.908 97.0135 175.059C74.2042 174.89 56.9538 167.575 45.7381 153.317C35.2355 139.966 29.8077 120.682 29.6052 96C29.8077 71.3178 35.2355 52.0336 45.7381 38.6827C56.9538 24.4249 74.2039 17.11 97.0132 16.9405C119.988 17.1113 137.539 24.4614 149.184 38.788C154.894 45.8136 159.199 54.6488 162.037 64.9503L178.184 60.6422C174.744 47.9622 169.331 37.0357 161.965 27.974C147.036 9.60668 125.202 0.195148 97.0695 0H96.9569C68.8816 0.19447 47.2921 9.6418 32.7883 28.0793C19.8819 44.4864 13.2244 67.3157 13.0007 95.9325L13 96L13.0007 96.0675C13.2244 124.684 19.8819 147.514 32.7883 163.921C47.2921 182.358 68.8816 191.806 96.9569 192H97.0695C122.03 191.827 139.624 185.292 154.118 170.811C173.081 151.866 172.51 128.119 166.26 113.541C161.776 103.087 153.227 94.5962 141.537 88.9883ZM98.4405 129.507C88.0005 130.095 77.1544 125.409 76.6196 115.372C76.2232 107.93 81.9158 99.626 99.0812 98.6368C101.047 98.5234 102.976 98.468 104.871 98.468C111.106 98.468 116.939 99.0737 122.242 100.233C120.264 124.935 108.662 128.946 98.4405 129.507Z\\\\\\\\\\\\\"></path></svg>\\\\\"},\\\\\"link\\\\\":\\\\\"https://www.threads.net/@md2rp\\\\\",\\\\\"ariaLabel\\\\\":\\\\\"Threads\\\\\"},{\\\\\"icon\\\\\":\\\\\"linkedin\\\\\",\\\\\"link\\\\\":\\\\\"https://www.linkedin.com/in/dennis-turp-494850173/\\\\\"}],\\\\\"footer\\\\\":{\\\\\"message\\\\\":\\\\\"Created by M. Dennis Turp\\\\\"}},\\\\\"locales\\\\\":{},\\\\\"scrollOffset\\\\\":90,\\\\\"cleanUrls\\\\\":false}\");')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://blog.mdturp.ch/posts/2024-04-05-visual_guide_to_vision_transformer.html\"\n",
    "\n",
    "headers_to_split_on = [\n",
    "    (\"h1\", \"Header 1\"),\n",
    "    (\"h2\", \"Header 2\"),\n",
    "    (\"h3\", \"Header 3\"),\n",
    "    (\"h4\", \"Header 4\")\n",
    "]\n",
    "\n",
    "html_splitter= HTMLHeaderTextSplitter(headers_to_split_on)\n",
    "split_docs= html_splitter.split_text_from_url(url)\n",
    "\n",
    "split_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b9e82b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "623f9687",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\erens\\AppData\\Local\\Temp\\ipykernel_14796\\1897406049.py:1: LangChainDeprecationWarning: The class `OllamaEmbeddings` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaEmbeddings``.\n",
      "  ollama_embeddings = OllamaEmbeddings(model=\"llama3.2\")\n"
     ]
    }
   ],
   "source": [
    "ollama_embeddings = OllamaEmbeddings(model=\"llama3.2\")\n",
    "db = FAISS.from_documents(split_docs, ollama_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c593c46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='197a89ff-a18a-44de-9874-4cc69ab8f33d', metadata={'Header 1': 'A Visual Guide to Vision Transformers', 'Header 3': '4) Flatting of the images patches'}, page_content='4) Flatting of the images patches'),\n",
       " Document(id='9b42612c-7c99-47f2-b1ce-bcd190a8ffc9', metadata={'Header 1': 'A Visual Guide to Vision Transformers', 'Header 3': '5) Creating patch embeddings'}, page_content='5) Creating patch embeddings'),\n",
       " Document(id='cdc24a1f-7854-416e-b45d-b1578ea50a74', metadata={'Header 1': 'A Visual Guide to Vision Transformers', 'Header 3': '6) Embedding all patches'}, page_content='6) Embedding all patches'),\n",
       " Document(id='9a028791-c82a-4dfc-9dc0-4b39b0fd277c', metadata={'Header 1': 'A Visual Guide to Vision Transformers', 'Header 3': '12) Identify Classification token output'}, page_content='12) Identify Classification token output')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"what is the effective way to train our model\"\n",
    "similarity = db.similarity_search(query)\n",
    "\n",
    "similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ceff5510",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\erens\\AppData\\Local\\Temp\\ipykernel_14796\\3504879169.py:2: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n"
     ]
    }
   ],
   "source": [
    "llm = OllamaEmbeddings(model=\"llama3.2\")\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4eef023",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\erens\\AppData\\Local\\Temp\\ipykernel_14796\\3960973058.py:5: LangChainDeprecationWarning: The class `ChatOllama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import ChatOllama``.\n",
      "  llm = ChatOllama(model=\"llama3.2\")\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "ollama_embeddings = OllamaEmbeddings()  \n",
    "llm = ChatOllama(model=\"llama3.2\")        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee62751c",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a179e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_history_aware_retriever\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4b43377",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import MessagesPlaceholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a78c6093",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prompt Template \n",
    "system_prompt = (\n",
    "    \"You are a highly capable AI assistant specializing in answering questions based on provided context.\"\n",
    "    \"Use only the given context to answer the question, and do not rely on prior knowledge or assumptions.\"\n",
    "    \"If the answer is not present in the context, clearly state, 'I don\\'t know.'\"\n",
    "    \"Keep your response concise, limited to a maximum of three sentences, and ensure clarity and relevance to the question.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "95a12aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': 'What is the purpose of Vision Transformers?', 'context': [Document(id='92b59d29-8996-4653-a1c9-67652b0b04ad', metadata={'Header 1': 'A Visual Guide to Vision Transformers'}, page_content='A Visual Guide to Vision Transformers'), Document(id='c2494ec1-f982-4ea6-9834-57cbfca53f8f', metadata={'Header 1': 'A Visual Guide to Vision Transformers', 'Header 3': '14) Training of the Vision Transformer'}, page_content='14) Training of the Vision Transformer'), Document(id='ec73269b-417b-4eb3-86e6-69fd271a810b', metadata={'Header 1': 'A Visual Guide to Vision Transformers', 'Header 3': '10.12)Transformer: Final Result'}, page_content='After the transformer step there is another residual connections which we will skip here for brevity. And so the last step concluded the transformer layer. In the end the transformer produced outputs of the same size as input.'), Document(id='e0c32b75-7935-4a9b-81ca-4270cb7af2c9', metadata={'Header 1': 'A Visual Guide to Vision Transformers', 'Header 3': '10.11)Transformer: Feed Forward Network'}, page_content='10.11)Transformer: Feed Forward Network')], 'answer': 'Based on the provided context, it appears that Vision Transformers are a type of neural network architecture designed for computer vision tasks, likely for image recognition and analysis. The exact purpose of Vision Transformers is not explicitly stated in the given context, but they seem to build upon the Transformer architecture, which suggests their application might involve sequential processing or attention mechanisms.'}\n",
      "Based on the provided context, it appears that Vision Transformers are a type of neural network architecture designed for computer vision tasks, likely for image recognition and analysis. The exact purpose of Vision Transformers is not explicitly stated in the given context, but they seem to build upon the Transformer architecture, which suggests their application might involve sequential processing or attention mechanisms.\n"
     ]
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "\n",
    "response = rag_chain.invoke({\"input\":\"What is the purpose of Vision Transformers?\"})\n",
    "\n",
    "#for index, res in enumerate(response, 1):\n",
    "   # print(f\"retrieval{index}\", res[\"answer\"])\n",
    "   \n",
    "print(response)\n",
    "print(response.get(\"answer\"))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e77f12a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "contextualize_q_system_prompt = (\n",
    "\"You are a question reformulation assistant. Given a chat history and the latest user \" \"question, your task is to reformulate the latest user question into a standalone question \"\"that can be understood without any reference to the previous conversation. \"\n",
    "\"If the question already makes complete sense on its own, return it as is. Do not answer the \"\n",
    "\"question. Your task is only to reformulate the question if necessary.\"\n",
    ")\n",
    "\n",
    "print(type(contextualize_q_system_prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4102bed4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableBranch(branches=[(RunnableLambda(lambda x: not x.get('chat_history', False)), RunnableLambda(lambda x: x['input'])\n",
       "| VectorStoreRetriever(tags=['FAISS', 'OllamaEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x0000029418AEE5D0>, search_kwargs={}))], default=ChatPromptTemplate(input_variables=['chat_history', 'input'], input_types={'chat_history': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x0000029417389E40>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a question reformulation assistant. Given a chat history and the latest user question, your task is to reformulate the latest user question into a standalone question that can be understood without any reference to the previous conversation. If the question already makes complete sense on its own, return it as is. Do not answer the question. Your task is only to reformulate the question if necessary.'), additional_kwargs={}), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])\n",
       "| ChatOllama(model='llama3.2')\n",
       "| StrOutputParser()\n",
       "| VectorStoreRetriever(tags=['FAISS', 'OllamaEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x0000029418AEE5D0>, search_kwargs={})), kwargs={}, config={'run_name': 'chat_retriever_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", contextualize_q_system_prompt),\n",
    "    MessagesPlaceholder(\"chat_history\"), \n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "#retrieval_chain = create_retrieval_chain(retriever, document_chain)\n",
    "\n",
    "history_aware_retriever = create_history_aware_retriever(llm, retriever, prompt)\n",
    "history_aware_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2e159090",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    MessagesPlaceholder(\"chat_history\"), \n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "207b5aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': 'What kind of embeddings are needed for vision transformers', 'chat_history': [HumanMessage(content='What is the purpose of vision transformers?', additional_kwargs={}, response_metadata={}), AIMessage(content='I don\\'t know. The provided context only mentions \"Training of the Vision Transformer\" and \"Transformer: Residual connections\", but it doesn\\'t explicitly state the purpose of vision transformers. However, based on general knowledge, vision transformers are a type of neural network architecture designed for image processing tasks.', additional_kwargs={}, response_metadata={})], 'context': [Document(id='f0c78451-4781-4b7a-a7f1-dc92f41ed0e5', metadata={'Header 1': 'A Visual Guide to Vision Transformers', 'Header 3': '1) Focus on one data point'}, page_content='To get a better understanding of what happens inside a vision transformer lets focus on a single data point (batch size of 1). And lets ask the question: How is this data point prepared in order to be consumed by a transformer?'), Document(id='ec73269b-417b-4eb3-86e6-69fd271a810b', metadata={'Header 1': 'A Visual Guide to Vision Transformers', 'Header 3': '10.12)Transformer: Final Result'}, page_content='After the transformer step there is another residual connections which we will skip here for brevity. And so the last step concluded the transformer layer. In the end the transformer produced outputs of the same size as input.'), Document(id='93a0e55e-db88-40af-8b4c-ca14ea7735ef', metadata={'Header 1': 'A Visual Guide to Vision Transformers'}, page_content='칙\\x80\\x8b  \\nThis is a visual guide to Vision Transformers (ViTs), a class of deep learning models that have achieved state-of-the-art performance on image classification tasks. Vision Transformers apply the transformer architecture, originally designed for natural language processing (NLP), to image data. This guide will walk you through the key components of Vision Transformers in a scroll story format, using visualizations and simple explanations to help you understand how these models work and how the flow of the data through the model looks like.'), Document(id='2c09cd0c-7e2d-4239-8715-4feab65b16c6', metadata={'Header 1': 'A Visual Guide to Vision Transformers', 'Header 3': '10.10)Transformer: Residual connection Result'}, page_content='The addition results in vectors of the same size.')], 'answer': \"I don't know. The provided context does not mention anything about the specific requirements or types of embeddings needed for vision transformers.\"}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage \n",
    "chat_history = []\n",
    "question = \"What is the purpose of vision transformers?\"\n",
    "response1 = rag_chain.invoke({\"input\": question, \"chat_history\": chat_history})\n",
    "chat_history.extend(\n",
    "    [\n",
    "        HumanMessage(content=question),\n",
    "        AIMessage(content=response1[\"answer\"])\n",
    "    ]\n",
    ")\n",
    "question2 = \"What kind of embeddings are needed for vision transformers\"\n",
    "response2 = rag_chain.invoke({\"input\": question2, \"chat_history\": chat_history})\n",
    "print(response2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6b286b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessage(content='What is the purpose of vision transformers?', additional_kwargs={}, response_metadata={}), AIMessage(content='I don\\'t know. The provided context only mentions \"Training of the Vision Transformer\" and \"Transformer: Residual connections\", but it doesn\\'t explicitly state the purpose of vision transformers. However, based on general knowledge, vision transformers are a type of neural network architecture designed for image processing tasks.', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "print(chat_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d5b9d3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "store = {}\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "conversational_rag_chain = RunnableWithMessageHistory(\n",
    "    rag_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    "    output_messages_key=\"answer\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fb42d1f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Vision transformers typically use patch embeddings. Patch embeddings involve dividing the input image into smaller patches, then embedding each patch separately before being concatenated to form the final sequence.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_rag_chain.invoke(\n",
    "    {\"input\": \"What kind of embeddings are needed for vision transformers\"},\n",
    "    config={\"configurable\":{\"session_id\": \"xyz123\"}}\n",
    ")[\"answer\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f9d7898c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I don't know. The context only mentions that vision transformers are trained in a supervised manner on a dataset of images and their corresponding labels, but it doesn't provide further details on the specifics of patch embeddings used in them.\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_rag_chain.invoke(\n",
    "    {\"input\": \"Tell me more about it?\"},\n",
    "    config={\"configurable\":{\"session_id\": \"xyz123\"}}\n",
    ")[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "33a7f0b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'xyz123': InMemoryChatMessageHistory(messages=[HumanMessage(content='What kind of embeddings are needed for vision transformers', additional_kwargs={}, response_metadata={}), AIMessage(content='Vision transformers typically use patch embeddings. Patch embeddings involve dividing the input image into smaller patches, then embedding each patch separately before being concatenated to form the final sequence.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Tell me more about it?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"I don't know. The context only mentions that vision transformers are trained in a supervised manner on a dataset of images and their corresponding labels, but it doesn't provide further details on the specifics of patch embeddings used in them.\", additional_kwargs={}, response_metadata={})])}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
